{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# HW3B - Pandas Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "See Canvas for details on how to complete and submit this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "This assignment transitions you from NumPy's numerical array operations to Pandas' powerful tabular data manipulation. While NumPy excels at homogeneous numerical arrays, Pandas is designed for the heterogeneous, labeled data that characterizes most real-world datasets—mixing dates, categories, numbers, and text within the same table.\n",
    "\n",
    "You'll work with real bike share data from Chicago's Divvy system to answer questions about urban transportation patterns. Through three progressively complex problems—exploring usage patterns, analyzing rider behavior, and conducting temporal analysis—you'll discover why Pandas has become the standard tool for data analysis in Python.\n",
    "\n",
    "The assignment emphasizes Pandas' design philosophy: named column access, explicit indexing methods (loc/iloc), handling missing data, and method chaining for readable data pipelines. You'll also see how Pandas builds on NumPy while adding the structure and convenience needed for practical data science work.\n",
    "\n",
    "This assignment should take 3-5 hours to complete.\n",
    "\n",
    "Before submitting, ensure your notebook:\n",
    "\n",
    "- Runs completely with \"Kernel → Restart & Run All\"\n",
    "- Includes thoughtful responses to all interpretation questions\n",
    "- Uses clear variable names and follows good coding practices\n",
    "- Shows your work (don't just print final answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "By completing this assignment, you will be able to:\n",
    "\n",
    "1. **Construct and manipulate Pandas data structures**\n",
    "   - Create DataFrames from dictionaries and CSV files\n",
    "   - Distinguish between Series and DataFrame objects\n",
    "   - Set and reset index structures appropriately\n",
    "   - Understand when operations return views vs copies\n",
    "2. **Apply explicit indexing paradigms**\n",
    "   - Use `loc[]` for label-based data access\n",
    "   - Use `iloc[]` for position-based data access\n",
    "   - Access columns using bracket notation\n",
    "   - Explain when each indexing method is appropriate\n",
    "3. **Diagnose and explore datasets systematically**\n",
    "   - Use `info()`, `describe()`, `head()`, and `dtypes` to understand data structure\n",
    "   - Identify missing values with `isna()` and `notna()`\n",
    "   - Calculate summary statistics across different axes\n",
    "   - Interpret value distributions with `value_counts()`\n",
    "4. **Filter data with boolean indexing and queries**\n",
    "   - Combine multiple conditions with `&`, `|`, and `~` operators\n",
    "   - Use `isin()` for membership testing\n",
    "   - Apply `query()` for readable complex filters\n",
    "   - Understand how index alignment affects operations\n",
    "5. **Work with datetime data**\n",
    "   - Parse dates during CSV loading\n",
    "   - Extract temporal components with the `.dt` accessor\n",
    "   - Filter data by date ranges\n",
    "   - Create time-based derived features\n",
    "6. **Connect Pandas patterns to data analysis workflows**\n",
    "   - Formulate questions that data can answer\n",
    "   - Choose appropriate methods for different analysis tasks\n",
    "   - Interpret results in domain context\n",
    "   - Recognize when vectorized operations outperform apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Generative AI Allowance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "You may use GenAI tools for brainstorming, explanations, and code sketches if you disclose it, understand it, and validate it. Your submission must represent your own work and you are solely responsible for its correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Total of 90 points available, will be graded out of 80. Scores of >100% are allowed.\n",
    "\n",
    "Distribution:\n",
    "\n",
    "- Tasks: 48 pts\n",
    "- Interpretation: 32 pts\n",
    "- Reflection: 10 pts\n",
    "\n",
    "Points by Problem:\n",
    "\n",
    "- Problem 1: 3 tasks, 10 pts\n",
    "- Problem 2: 4 tasks, 14 pts\n",
    "- Problem 3: 4 tasks, 14 pts\n",
    "- Problem 4: 3 tasks, 10 pts\n",
    "\n",
    "Interpretation Questions:\n",
    "\n",
    "- Problem 1: 3 questions, 8 pts\n",
    "- Problem 2: 4 questions, 8 pts\n",
    "- Problem 3: 3 questions, 8 pts\n",
    "- Problem 4: 3 questions, 8 pts\n",
    "\n",
    "Graduate differentiation: poor follow-up responses will result in up to a 5pt deduction for that problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Dataset: Chicago Divvy Bike Share"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "The dataset you will analyze is based on real trip information from Divvy, Chicago's bike share system. It contains individual trips with start/end times, station information, and rider type.\n",
    "\n",
    "Dataset homepage: https://divvybikes.com/system-data\n",
    "\n",
    "Each trip includes:\n",
    "\n",
    "- Trip start and end times (datetime)\n",
    "- Start and end station names and IDs\n",
    "- Rider type (member vs casual)\n",
    "- Bike type (classic, electric, or docked)\n",
    "\n",
    "Chicago's Department of Transportation uses this data to optimize station placement, understand usage patterns, and improve service. You'll explore similar questions that real transportation analysts investigate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Problem 1: Creating DataFrames from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Before loading data from files, you need to understand how Pandas structures are built. In this problem, you'll create Series and DataFrames manually using Python's built-in data structures. This is a quick warmup to establish the fundamentals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "#### Task 1a: Create a Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Create a Series called `temperatures` representing daily high temperatures for a week:\n",
    "\n",
    "- Monday: 72°F\n",
    "- Tuesday: 75°F  \n",
    "- Wednesday: 68°F\n",
    "- Thursday: 71°F\n",
    "- Friday: 73°F\n",
    "\n",
    "Use the day names as the index. Print the Series and its data type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monday       72\n",
      "Tuesday      75\n",
      "Wednesday    68\n",
      "Thursday     71\n",
      "Friday       73\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "index = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']\n",
    "data = [72, 75, 68, 71, 73]\n",
    "\n",
    "temperatures = pd.Series(data, index=index)\n",
    "\n",
    "print(temperatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "#### Task 1b: Create a DataFrame from a Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Create a DataFrame called `products` with the following data:\n",
    "\n",
    "| product | price | quantity |\n",
    "|---------|-------|----------|\n",
    "| Widget  | 19.99 | 100 |\n",
    "| Gadget  | 24.99 | 75 |\n",
    "| Doohickey | 12.49 | 150 |\n",
    "\n",
    "Use a dictionary where keys are column names and values are lists. Print the DataFrame and report its shape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     product  price  quantity\n",
      "0     Widget  19.99       100\n",
      "1     Gadget  24.99        75\n",
      "2  Doohickey  12.49       150\n",
      "Shape: (3, 3)\n"
     ]
    }
   ],
   "source": [
    "product = ['Widget', 'Gadget', 'Doohickey']\n",
    "price = [19.99, 24.99, 12.49]\n",
    "quantity = [100, 75, 150]\n",
    "\n",
    "products = {'product':product, 'price':price, 'quantity':quantity}\n",
    "products = pd.DataFrame(products)\n",
    "\n",
    "print(products)\n",
    "print(f'Shape: {products.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "#### Task 1c: Access DataFrame Elements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Using the `products` DataFrame from Task 1b, extract and print:\n",
    "\n",
    "1. The `price` column as a Series\n",
    "2. The `product` and `quantity` columns as a DataFrame (using a list of column names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    19.99\n",
      "1    24.99\n",
      "2    12.49\n",
      "Name: price, dtype: float64\n",
      "     product  quantity\n",
      "0     Widget       100\n",
      "1     Gadget        75\n",
      "2  Doohickey       150\n"
     ]
    }
   ],
   "source": [
    "#series\n",
    "prices = products['price']\n",
    "print(prices)\n",
    "\n",
    "#the extra set of [] makes this a data frame\n",
    "product_and_quantity = products[['product', 'quantity']]\n",
    "print(product_and_quantity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "#### Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Answer the following questions (briefly / concisely) in the markdown cell below:\n",
    "\n",
    "1. Data structure mapping: When you create a DataFrame from a dictionary (like in Task 1b), what do the dictionary keys become? What do the values become?\n",
    "2. Bracket notation: Why does `df['price']` return a Series, but `df[['price']]` return a DataFrame? What's the difference in what you're asking for?\n",
    "3. Index purpose: In Task 1a, you used day names as the index instead of default numbers (0, 1, 2...). When would a custom index like this be more useful than the default numeric index?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "##### Your Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "1. The dictionary keys become the column headers and the values become the columns.\n",
    "2. The second set of brackets specifies that you want a data frame because of pandas' indexing rules.\n",
    "3. A custom index like 1a would be helpful when knowing what columns you want but not what number they are. For example, if I'm working with the penguin data set, I may know that I want bill size and weight, but I don't know what column number they are in the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Problem 2: Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "Before starting this problem, make sure you are working in a copy of this file in the `my_repo` folder you created in HW2a. You must also have a copy of the file `202410-divvy-tripdata-100k.csv` in a subdirectory called `data`. That file structure is illustrated below.\n",
    "\n",
    "```text\n",
    "~/insy6500/my_repo\n",
    "└── homework\n",
    "    ├── data\n",
    "    │   └── 202410-divvy-tripdata-100k.csv\n",
    "    └── hw3b.ipynb\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "#### Task 2a: Load and Understand Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "Start by loading the data \"as-is\" to get a general understanding of the overall structure and how Pandas interprets it by default.\n",
    "\n",
    "Note on file paths: The provided code uses `Path` from Python's `pathlib` module to handle file paths. Path objects work consistently across operating systems (Windows uses backslashes `\\`, Mac/Linux use forward slashes `/`), automatically using the correct separator for your system. The provided code defines `csv_path` which should be used as the filename in your `pd.read_csv` to load the data file.\n",
    "\n",
    "1. Use `pd.read_csv` to load `csv_path` (provided below) without specifying any other arguments. Assign it to the variable `df_raw`.\n",
    "2. Use the methods we described in class to explore the shape, structure, types, etc. of the data. In particular, consider which columns represent dates or categories.\n",
    "3. Note the amount of memory used by the dataset. See the section on memory diagnostics in notebook 07a for appropriate code snippets using `memory_usage`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 13 columns):\n",
      " #   Column              Non-Null Count   Dtype  \n",
      "---  ------              --------------   -----  \n",
      " 0   ride_id             100000 non-null  object \n",
      " 1   rideable_type       100000 non-null  object \n",
      " 2   started_at          100000 non-null  object \n",
      " 3   ended_at            100000 non-null  object \n",
      " 4   start_station_name  89623 non-null   object \n",
      " 5   start_station_id    89623 non-null   object \n",
      " 6   end_station_name    89485 non-null   object \n",
      " 7   end_station_id      89485 non-null   object \n",
      " 8   start_lat           100000 non-null  float64\n",
      " 9   start_lng           100000 non-null  float64\n",
      " 10  end_lat             99913 non-null   float64\n",
      " 11  end_lng             99913 non-null   float64\n",
      " 12  member_casual       100000 non-null  object \n",
      "dtypes: float64(4), object(9)\n",
      "memory usage: 9.9+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# create a OS-independent pointer to the csv file created by Setup\n",
    "csv_path = Path('./data/202410-divvy-tripdata-100k.csv')\n",
    "\n",
    "# load and explore the data below (create additional code / markdown cells as necessary)\n",
    "df_raw = pd.read_csv(csv_path)\n",
    "\n",
    "#get some basic info\n",
    "df_raw.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a8352d8-9866-4547-a86a-02007cf07a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ride_id</th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>started_at</th>\n",
       "      <th>ended_at</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lng</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lng</th>\n",
       "      <th>member_casual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6148</th>\n",
       "      <td>C9EF5F85A1F29092</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-10-02 17:29:06.694</td>\n",
       "      <td>2024-10-02 17:34:38.477</td>\n",
       "      <td>Elizabeth St &amp; Randolph St</td>\n",
       "      <td>53ddd5</td>\n",
       "      <td>Sangamon St &amp; Lake St</td>\n",
       "      <td>2fda38</td>\n",
       "      <td>41.884336</td>\n",
       "      <td>-87.658902</td>\n",
       "      <td>41.885779</td>\n",
       "      <td>-87.651025</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44145</th>\n",
       "      <td>1F1D121D289F0A69</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-10-13 00:12:16.582</td>\n",
       "      <td>2024-10-13 00:31:05.307</td>\n",
       "      <td>Green St &amp; Randolph St*</td>\n",
       "      <td>ef9ebf</td>\n",
       "      <td>Broadway &amp; Cornelia Ave</td>\n",
       "      <td>0638e8</td>\n",
       "      <td>41.883602</td>\n",
       "      <td>-87.648627</td>\n",
       "      <td>41.945529</td>\n",
       "      <td>-87.646439</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38310</th>\n",
       "      <td>88C8B9E3FC6AAEC9</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-10-11 16:06:35.420</td>\n",
       "      <td>2024-10-11 16:16:47.324</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Calumet Ave &amp; 18th St</td>\n",
       "      <td>2a91cc</td>\n",
       "      <td>41.880000</td>\n",
       "      <td>-87.630000</td>\n",
       "      <td>41.857618</td>\n",
       "      <td>-87.619411</td>\n",
       "      <td>casual</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ride_id  rideable_type               started_at  \\\n",
       "6148   C9EF5F85A1F29092  electric_bike  2024-10-02 17:29:06.694   \n",
       "44145  1F1D121D289F0A69  electric_bike  2024-10-13 00:12:16.582   \n",
       "38310  88C8B9E3FC6AAEC9  electric_bike  2024-10-11 16:06:35.420   \n",
       "\n",
       "                      ended_at          start_station_name start_station_id  \\\n",
       "6148   2024-10-02 17:34:38.477  Elizabeth St & Randolph St           53ddd5   \n",
       "44145  2024-10-13 00:31:05.307     Green St & Randolph St*           ef9ebf   \n",
       "38310  2024-10-11 16:16:47.324                         NaN              NaN   \n",
       "\n",
       "              end_station_name end_station_id  start_lat  start_lng  \\\n",
       "6148     Sangamon St & Lake St         2fda38  41.884336 -87.658902   \n",
       "44145  Broadway & Cornelia Ave         0638e8  41.883602 -87.648627   \n",
       "38310    Calumet Ave & 18th St         2a91cc  41.880000 -87.630000   \n",
       "\n",
       "         end_lat    end_lng member_casual  \n",
       "6148   41.885779 -87.651025        member  \n",
       "44145  41.945529 -87.646439        member  \n",
       "38310  41.857618 -87.619411        casual  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#exploring some other info here\n",
    "\n",
    "#how big is this thing?\n",
    "df_raw.shape\n",
    "\n",
    "#quite large. i think the date columns are started_at and ended_at, but i'm going to pull a sample just to be sure:\n",
    "df_raw.sample(n=3)\n",
    "#this tells me that rideable_type and member_casual are categories, and started_at and ended_at contain dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6380a1f-a240-466c-9c9b-e67e6fc9a255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.70 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"{df_raw.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "#### Task 2b: Reload with Proper Data Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "1. Repeat step 2a.1 to reload the data. Use the `dtype` and `parse_dates` arguments to properly assign categorical and date types. Assign the result to the variable name `rides`.\n",
    "2. After loading, use `rides.info()` to confirm the type changes.\n",
    "3. Use `memory_usage` to compare the resulting size with that from step 2a.3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 13 columns):\n",
      " #   Column              Non-Null Count   Dtype         \n",
      "---  ------              --------------   -----         \n",
      " 0   ride_id             100000 non-null  object        \n",
      " 1   rideable_type       100000 non-null  category      \n",
      " 2   started_at          100000 non-null  datetime64[ns]\n",
      " 3   ended_at            100000 non-null  datetime64[ns]\n",
      " 4   start_station_name  89623 non-null   object        \n",
      " 5   start_station_id    89623 non-null   object        \n",
      " 6   end_station_name    89485 non-null   object        \n",
      " 7   end_station_id      89485 non-null   object        \n",
      " 8   start_lat           100000 non-null  float64       \n",
      " 9   start_lng           100000 non-null  float64       \n",
      " 10  end_lat             99913 non-null   float64       \n",
      " 11  end_lng             99913 non-null   float64       \n",
      " 12  member_casual       100000 non-null  category      \n",
      "dtypes: category(2), datetime64[ns](2), float64(4), object(5)\n",
      "memory usage: 8.6+ MB\n",
      "37.53 MB\n"
     ]
    }
   ],
   "source": [
    "# task 2b code here...\n",
    "rides = pd.read_csv(csv_path, parse_dates=['started_at', 'ended_at'], dtype={'rideable_type':'category', 'member_casual':'category'})\n",
    "rides.info()\n",
    "print(f\"{rides.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "#### Task 2c: Explore Structure and Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "Using the `rides` DataFrame from Task 2b:\n",
    "\n",
    "1. Determine the range of starting dates in the dataframe using the `min` and `max` methods.\n",
    "2. Count the number of missing values in each column. See the section of the same name in lecture 06b.\n",
    "3. Convert the Series from step 2 to a DataFrame using `.to_frame(name='count')`, then add a column called 'percentage' that calculates the percentage of missing values for each column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start date range: 31 days 00:42:01.229000\n",
      "                    count percentage\n",
      "ride_id                 0      0.00%\n",
      "rideable_type           0      0.00%\n",
      "started_at              0      0.00%\n",
      "ended_at                0      0.00%\n",
      "start_station_name  10377     11.58%\n",
      "start_station_id    10377     11.58%\n",
      "end_station_name    10515     11.75%\n",
      "end_station_id      10515     11.75%\n",
      "start_lat               0      0.00%\n",
      "start_lng               0      0.00%\n",
      "end_lat                87      0.09%\n",
      "end_lng                87      0.09%\n",
      "member_casual           0      0.00%\n"
     ]
    }
   ],
   "source": [
    "# task 2c code here...\n",
    "startdate_range = rides['started_at'].max() - rides['started_at'].min()\n",
    "print(f'Start date range: {startdate_range}')\n",
    "\n",
    "missing_values = rides.isna().sum()\n",
    "\n",
    "new_df = missing_values.to_frame(name='count')\n",
    "\n",
    "percentage = ((rides.isna().sum() / rides.count()) * 100)\n",
    "\n",
    "#this lambda application came from perplexity ai to get the formatting that i wanted\n",
    "new_df['percentage'] = percentage.apply(lambda x: f\"{x:.2f}%\")\n",
    "\n",
    "print(new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "#### Task 2d: Create Trip Duration Column and Set Index\n",
    "\n",
    "Before setting the index, create a derived column for trip duration:\n",
    "\n",
    "1. Calculate trip_duration_min by subtracting `started_at` from `ended_at`, then converting to minutes using `.dt.total_seconds() / 60`\n",
    "3. Display basic statistics (min, max, mean) for the new column using `.describe()`\n",
    "4. Show the first few rows with `started_at`, `ended_at`, and `trip_duration_min` to verify the calculation\n",
    "5. Set `started_at` as the DataFrame's index. Verify the change by printing the index and displaying the first few rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5a47a99-006f-4198-9d65-a1cdc1249c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    100000.000000\n",
       "mean         16.144576\n",
       "std          52.922539\n",
       "min           0.006533\n",
       "25%           5.489271\n",
       "50%           9.423592\n",
       "75%          16.407171\n",
       "max        1499.949717\n",
       "Name: trip_duration_min, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# task 2d code here...\n",
    "\n",
    "#part 1 & 2\n",
    "rides['trip_duration_min'] = (rides['ended_at'] - rides['started_at']).dt.total_seconds() / 60\n",
    "rides['trip_duration_min'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92b15cf4-60c6-44e0-8311-8a2a579d7bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>started_at</th>\n",
       "      <th>ended_at</th>\n",
       "      <th>trip_duration_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>97644</th>\n",
       "      <td>2024-10-31 00:05:54.613</td>\n",
       "      <td>2024-10-31 00:10:10.044</td>\n",
       "      <td>4.257183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91359</th>\n",
       "      <td>2024-10-29 08:11:44.185</td>\n",
       "      <td>2024-10-29 08:16:06.922</td>\n",
       "      <td>4.378950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48333</th>\n",
       "      <td>2024-10-14 08:51:09.593</td>\n",
       "      <td>2024-10-14 09:01:35.315</td>\n",
       "      <td>10.428700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38694</th>\n",
       "      <td>2024-10-11 17:02:15.349</td>\n",
       "      <td>2024-10-11 17:08:22.669</td>\n",
       "      <td>6.122000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89691</th>\n",
       "      <td>2024-10-28 15:46:23.076</td>\n",
       "      <td>2024-10-28 15:56:43.189</td>\n",
       "      <td>10.335217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   started_at                ended_at  trip_duration_min\n",
       "97644 2024-10-31 00:05:54.613 2024-10-31 00:10:10.044           4.257183\n",
       "91359 2024-10-29 08:11:44.185 2024-10-29 08:16:06.922           4.378950\n",
       "48333 2024-10-14 08:51:09.593 2024-10-14 09:01:35.315          10.428700\n",
       "38694 2024-10-11 17:02:15.349 2024-10-11 17:08:22.669           6.122000\n",
       "89691 2024-10-28 15:46:23.076 2024-10-28 15:56:43.189          10.335217"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#part 3\n",
    "columns = ['started_at', 'ended_at', 'trip_duration_min']\n",
    "rides[columns].sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4089aace-e014-4549-b4a0-605605a52159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Index: DatetimeIndex(['2024-09-30 23:12:01.622000', '2024-09-30 23:19:25.409000',\n",
      "               '2024-09-30 23:32:24.672000', '2024-09-30 23:42:11.207000',\n",
      "               '2024-09-30 23:49:25.380000', '2024-09-30 23:49:40.016000',\n",
      "               '2024-10-01 00:00:53.414000', '2024-10-01 00:05:44.954000',\n",
      "               '2024-10-01 00:06:12.035000', '2024-10-01 00:10:03.646000',\n",
      "               ...\n",
      "               '2024-10-31 23:36:04.200000', '2024-10-31 23:36:34.956000',\n",
      "               '2024-10-31 23:36:49.500000', '2024-10-31 23:38:20.262000',\n",
      "               '2024-10-31 23:44:03.832000', '2024-10-31 23:44:23.211000',\n",
      "               '2024-10-31 23:44:45.948000', '2024-10-31 23:50:31.160000',\n",
      "               '2024-10-31 23:53:02.355000', '2024-10-31 23:54:02.851000'],\n",
      "              dtype='datetime64[ns]', name='started_at', length=100000, freq=None)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ride_id</th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>ended_at</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lng</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lng</th>\n",
       "      <th>member_casual</th>\n",
       "      <th>trip_duration_min</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>started_at</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-10-25 12:46:45.422</th>\n",
       "      <td>7AA9E1706530F8EC</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-10-25 13:16:03.640</td>\n",
       "      <td>Clinton St &amp; Jackson Blvd</td>\n",
       "      <td>542222</td>\n",
       "      <td>Pine Grove Ave &amp; Irving Park Rd</td>\n",
       "      <td>d710e0</td>\n",
       "      <td>41.878317</td>\n",
       "      <td>-87.640981</td>\n",
       "      <td>41.954383</td>\n",
       "      <td>-87.648043</td>\n",
       "      <td>member</td>\n",
       "      <td>29.303633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-16 10:59:43.304</th>\n",
       "      <td>3F0504118C5E7340</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-10-16 11:22:56.305</td>\n",
       "      <td>Clark St &amp; Lake St</td>\n",
       "      <td>b1148b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.886021</td>\n",
       "      <td>-87.630876</td>\n",
       "      <td>41.890000</td>\n",
       "      <td>-87.620000</td>\n",
       "      <td>member</td>\n",
       "      <td>23.216683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-28 19:50:47.663</th>\n",
       "      <td>6206A55AABB1B9D7</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-10-28 19:58:56.572</td>\n",
       "      <td>Winthrop Ave &amp; Lawrence Ave</td>\n",
       "      <td>f7c44f</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.968812</td>\n",
       "      <td>-87.657659</td>\n",
       "      <td>41.990000</td>\n",
       "      <td>-87.670000</td>\n",
       "      <td>member</td>\n",
       "      <td>8.148483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-03 19:04:01.852</th>\n",
       "      <td>76192891D29FB30B</td>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2024-10-03 19:09:27.800</td>\n",
       "      <td>Sheffield Ave &amp; Waveland Ave</td>\n",
       "      <td>d19b90</td>\n",
       "      <td>Southport Ave &amp; Irving Park Rd</td>\n",
       "      <td>20be27</td>\n",
       "      <td>41.949399</td>\n",
       "      <td>-87.654529</td>\n",
       "      <td>41.954177</td>\n",
       "      <td>-87.664358</td>\n",
       "      <td>member</td>\n",
       "      <td>5.432467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-10-11 08:45:27.837</th>\n",
       "      <td>380EF1A30AEE06F9</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-10-11 08:48:42.767</td>\n",
       "      <td>Sheffield Ave &amp; Kingsbury St</td>\n",
       "      <td>45a800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41.910522</td>\n",
       "      <td>-87.653106</td>\n",
       "      <td>41.920000</td>\n",
       "      <td>-87.650000</td>\n",
       "      <td>casual</td>\n",
       "      <td>3.248833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  ride_id  rideable_type  \\\n",
       "started_at                                                 \n",
       "2024-10-25 12:46:45.422  7AA9E1706530F8EC  electric_bike   \n",
       "2024-10-16 10:59:43.304  3F0504118C5E7340  electric_bike   \n",
       "2024-10-28 19:50:47.663  6206A55AABB1B9D7  electric_bike   \n",
       "2024-10-03 19:04:01.852  76192891D29FB30B   classic_bike   \n",
       "2024-10-11 08:45:27.837  380EF1A30AEE06F9  electric_bike   \n",
       "\n",
       "                                       ended_at            start_station_name  \\\n",
       "started_at                                                                      \n",
       "2024-10-25 12:46:45.422 2024-10-25 13:16:03.640     Clinton St & Jackson Blvd   \n",
       "2024-10-16 10:59:43.304 2024-10-16 11:22:56.305            Clark St & Lake St   \n",
       "2024-10-28 19:50:47.663 2024-10-28 19:58:56.572   Winthrop Ave & Lawrence Ave   \n",
       "2024-10-03 19:04:01.852 2024-10-03 19:09:27.800  Sheffield Ave & Waveland Ave   \n",
       "2024-10-11 08:45:27.837 2024-10-11 08:48:42.767  Sheffield Ave & Kingsbury St   \n",
       "\n",
       "                        start_station_id                 end_station_name  \\\n",
       "started_at                                                                  \n",
       "2024-10-25 12:46:45.422           542222  Pine Grove Ave & Irving Park Rd   \n",
       "2024-10-16 10:59:43.304           b1148b                              NaN   \n",
       "2024-10-28 19:50:47.663           f7c44f                              NaN   \n",
       "2024-10-03 19:04:01.852           d19b90   Southport Ave & Irving Park Rd   \n",
       "2024-10-11 08:45:27.837           45a800                              NaN   \n",
       "\n",
       "                        end_station_id  start_lat  start_lng    end_lat  \\\n",
       "started_at                                                                \n",
       "2024-10-25 12:46:45.422         d710e0  41.878317 -87.640981  41.954383   \n",
       "2024-10-16 10:59:43.304            NaN  41.886021 -87.630876  41.890000   \n",
       "2024-10-28 19:50:47.663            NaN  41.968812 -87.657659  41.990000   \n",
       "2024-10-03 19:04:01.852         20be27  41.949399 -87.654529  41.954177   \n",
       "2024-10-11 08:45:27.837            NaN  41.910522 -87.653106  41.920000   \n",
       "\n",
       "                           end_lng member_casual  trip_duration_min  \n",
       "started_at                                                           \n",
       "2024-10-25 12:46:45.422 -87.648043        member          29.303633  \n",
       "2024-10-16 10:59:43.304 -87.620000        member          23.216683  \n",
       "2024-10-28 19:50:47.663 -87.670000        member           8.148483  \n",
       "2024-10-03 19:04:01.852 -87.664358        member           5.432467  \n",
       "2024-10-11 08:45:27.837 -87.650000        casual           3.248833  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#part 4\n",
    "rides = rides.set_index('started_at')\n",
    "print(\"\\nIndex:\", rides.index)\n",
    "rides.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "#### Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "Reflect on problem 2 and answer (briefly / concisely) the following questions:\n",
    "\n",
    "1. What types did Pandas assign to `started_at` and `member_casual` in Task 2a? Why might these defaults be problematic?\n",
    "2. Look at the values in the station ID fields. Based on what you learned about git commit IDs in HW3a, how do you think the station IDs were derived?\n",
    "3. Explain in your own words what method chaining is, what `df.isna().sum()` does and how it works.\n",
    "4. Assume you found ~10% missing values in station columns but ~0% in coordinates. What might explain this? How might you handle the affected rows?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "##### Your Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "1. It had originally assigned object to both of these. With the date columns being objets and not datetime, there are certain operations that would not work. As for member_casual, the dtype being object rather than category will use an unnecessarily large amount of memory.\n",
    "2. They are likely produced on behalf of some type of algorithm and designed to be an unique identifier for each station.\n",
    "3. Method chaining is combining multiple methods into a \"chain\" that are completed in the order they are read. For example, df.isna().sum() takes the data frame (df), finds the na values and returns true or false values, and then finds the sum of all true values.\n",
    "4. Depending on how this data was collected, it's possible that either the coordinates were automatically collected but the station names had to be manually typed in and 10% of people did not type them, or it's also possible that not every starting location was at a station. When you think about these e-bikes and scooters, occasionally they are left on sidewalks or corners, which wouldn't be an assigned station, but would still have coordinates attached geographically. Depending on what the reasoning is for the misaligned data and what resources I have, I might do a search and report function to back fill the missing rows in relation to other stations already reported."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "#### Follow-Up (Graduate Students Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "Compare memory usage results in 2a.3 and 2b.3. What caused the change? Why are these numbers different from what is reported at the bottom of `df.info()`? Which should you use if data size is a concern?\n",
    "\n",
    "Working with DataFrames typically requires 5-10x the dataset size in available RAM. On a system with 16GB, assuming about 30% overhead from the operating system and other programs, what range of dataset sizes would be safely manageable? Calculate using both 5x (optimistic) and 10x (conservative) multipliers, then explain which you'd recommend for reliable work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "##### Your Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "The change in memory usage is on account of us changing the dtype of four columns to datetime and category. Changing the dtype of a column from object to category can save significantly on memory usage. The values between our calculation and what is shown at the bottom of the info page differ because what is shown in info is an estimation that does not take actual values into account. If data size is a concern, you should use the calculation because the estimation provided by info is typically far under the actual usage value.\n",
    "\n",
    "On a system with 16GB, 30% overhead leaves 11.2GB. With the 5x multiplier, that leaves room for datasets that are about 2.24GB. The 10x multipler leaves room for datasets that are about 1.12GB. I would recommend datasets smaller than 1.12GB for reliable work, as we know operating at a higher than recommended memory can overburden our machines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "### Problem 3: Filtering and Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "With clean data loaded, you can now filter and transform it to answer specific questions. This problem focuses on Pandas' powerful indexing and filtering capabilities, along with creating derived columns that enable deeper analysis.\n",
    "\n",
    "You'll continue working with the `rides` DataFrame from Problem 2, which has `started_at` set as the index."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "#### Task 3a: Boolean Indexing and Membership Testing\n",
    "\n",
    "Use boolean indexing and the `isin()` method to answer these questions:\n",
    "\n",
    "1. How many trips were taken by *members* using *electric bikes*? Use `&` to combine conditions.\n",
    "2. What percentage of all trips does this represent?\n",
    "3. How many trips started at any of these three stations: \"Streeter Dr & Grand Ave\", \"DuSable Lake Shore Dr & Monroe St\", or \"Kingsbury St & Kinzie St\"? Use `isin()`.\n",
    "\n",
    "Note: Remember to use parentheses around each condition when combining with `&`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trips taken by members on electric bikes: 33121\n",
      "Percentage of trips: 33.121%\n",
      "Number of trips at three stations: 2702\n"
     ]
    }
   ],
   "source": [
    "# Task 3a code here...\n",
    "members_bikes = rides[(rides['rideable_type'] == 'electric_bike') & (rides['member_casual'] == 'member')]['ride_id'].count()\n",
    "print(f'Number of trips taken by members on electric bikes: {members_bikes}')\n",
    "\n",
    "all_trips = rides['ride_id'].count()\n",
    "percentage = (members_bikes / all_trips) * 100\n",
    "print(f'Percentage of trips: {percentage}%')\n",
    "\n",
    "stations = [\"Streeter Dr & Grand Ave\", \"DuSable Lake Shore Dr & Monroe St\", \"Kingsbury St & Kinzie St\"]\n",
    "number_of_trips = rides['start_station_name'].isin(stations)\n",
    "print(f'Number of trips at three stations: {number_of_trips.sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "#### Task 3b: Create Derived Columns from Datetime\n",
    "\n",
    "Add two categorical columns to the rides DataFrame based on trip start time:\n",
    "\n",
    "1. `is_weekend`: Boolean column that is True for Saturday/Sunday trips. Use .dt.dayofweek on the index (Monday=0, Sunday=6).\n",
    "2. `time_of_day`: String categories based on start hour:\n",
    "   - \"Morning Rush\" if hour is 7, 8, or 9\n",
    "   - \"Evening Rush\" if hour is 16, 17, or 18\n",
    "   - \"Midday\" for all other hours\n",
    "\n",
    "For step 2, initialize the column to \"Midday\", then use .loc[mask, 'time_of_day'] with boolean masks to assign rush hour categories. Extract hour using .dt.hour on the index.\n",
    "\n",
    "After creating both columns, use value_counts() on time_of_day to show the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time_of_day\n",
       "Midday          55912\n",
       "Evening Rush    28218\n",
       "Morning Rush    15870\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task 3b code here...\n",
    "rides['is_weekend'] = rides.index.dayofweek.isin([5, 6])\n",
    "\n",
    "morning_rush = rides.index.hour.isin([7, 8, 9])\n",
    "evening_rush = rides.index.hour.isin([16, 17, 18])\n",
    "\n",
    "rides['time_of_day'] = 'Midday'\n",
    "rides.loc[morning_rush, 'time_of_day'] = 'Morning Rush'\n",
    "rides.loc[evening_rush, 'time_of_day'] = 'Evening Rush'\n",
    "#i printed a sample of rides here to make sure it was behaving as expected & it was\n",
    "\n",
    "rides['time_of_day'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "#### Task 3c: Complex Filtering with query()\n",
    "\n",
    "Use the `query()` method to find trips that meet **all** of these criteria:\n",
    "- Casual riders (not members)\n",
    "- Weekend trips  \n",
    "- Duration greater than 20 minutes\n",
    "- Electric bikes\n",
    "\n",
    "Report:\n",
    "1. How many trips match these criteria?\n",
    "2. What percentage of all trips do they represent?\n",
    "3. What is the average duration of these trips?\n",
    "\n",
    "Hint: Column names work directly in `query()` strings. Combine conditions with `and`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rides that meet criteria: 1501\n",
      "Percentage of all trips: 1.50%\n",
      "Average Duration: 40.37 minutes\n"
     ]
    }
   ],
   "source": [
    "# Task 3c code here...\n",
    "specified_rides = rides.query('member_casual == \"casual\" and is_weekend == True and trip_duration_min > 20 and rideable_type == \"electric_bike\"')\n",
    "\n",
    "print(f\"Number of rides that meet criteria: {specified_rides['ride_id'].count()}\")\n",
    "print(f\"Percentage of all trips: {((specified_rides['ride_id'].count() / rides['ride_id'].count()) * 100):.2f}%\")\n",
    "print(f\"Average Duration: {specified_rides['trip_duration_min'].mean():.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "#### Task 3d: Explicit Indexing Practice\n",
    "\n",
    "Practice using `loc[]` and `iloc[]` for different selection tasks:\n",
    "\n",
    "1. Use `iloc[]` to select the first 10 trips, showing only `member_casual`, `rideable_type`, and `trip_duration_min` columns\n",
    "2. Use `loc[]` to select trips from October 15-17 (use date strings `'2024-10-15':'2024-10-17'`), showing the same three columns\n",
    "3. Count how many trips occurred during this date range\n",
    "\n",
    "Note: When using `iloc[]`, remember it's position-based (0-indexed). When using `loc[]` with the datetime index, you can slice using date strings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         rideable_type  trip_duration_min  is_weekend\n",
      "started_at                                                           \n",
      "2024-09-30 23:12:01.622  electric_bike          67.984200       False\n",
      "2024-09-30 23:19:25.409  electric_bike          82.742067       False\n",
      "2024-09-30 23:32:24.672   classic_bike          50.899583       False\n",
      "2024-09-30 23:42:11.207   classic_bike          28.093733       False\n",
      "2024-09-30 23:49:25.380  electric_bike          17.034933       False\n",
      "2024-09-30 23:49:40.016  electric_bike          13.009367       False\n",
      "2024-10-01 00:00:53.414   classic_bike           2.598817       False\n",
      "2024-10-01 00:05:44.954  electric_bike           0.013433       False\n",
      "2024-10-01 00:06:12.035  electric_bike          10.472933       False\n",
      "2024-10-01 00:10:03.646   classic_bike           7.825683       False\n",
      "                        member_casual  rideable_type  trip_duration_min\n",
      "started_at                                                             \n",
      "2024-10-15 00:00:12.781        casual  electric_bike           2.804233\n",
      "2024-10-15 00:01:20.517        member  electric_bike          11.330867\n",
      "2024-10-15 00:05:24.811        member  electric_bike           1.868850\n",
      "2024-10-15 00:05:52.984        member  electric_bike           2.705083\n",
      "2024-10-15 00:06:18.819        member  electric_bike           1.600867\n",
      "...                               ...            ...                ...\n",
      "2024-10-17 23:45:48.739        member  electric_bike          14.703100\n",
      "2024-10-17 23:47:35.040        member  electric_bike          13.049867\n",
      "2024-10-17 23:55:34.112        member   classic_bike           3.795400\n",
      "2024-10-17 23:56:14.464        member  electric_bike          11.937217\n",
      "2024-10-17 23:59:38.103        casual   classic_bike           5.266433\n",
      "\n",
      "[7235 rows x 3 columns]\n",
      "Number of trips from October 15-17: 7235\n"
     ]
    }
   ],
   "source": [
    "# Task 3d code here...\n",
    "print(rides.iloc[0:10, [1, 12, 13]])\n",
    "\n",
    "print(rides.loc['2024-10-15':'2024-10-17', ['member_casual', 'rideable_type', 'trip_duration_min']])\n",
    "#here i had to do some brainstorming because it was originally returning an empty data frame but then i realized it was because i had set the index on a separate df (rides_dated), so i had to go back and change that\n",
    "print(f\"Number of trips from October 15-17: {rides.loc['2024-10-15':'2024-10-17']['ride_id'].count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "#### Interpretation\n",
    "\n",
    "Reflect on this problem and answer (briefly / concisely) the following questions:\n",
    "\n",
    "1. `isin()` advantages: Compare using `isin(['A', 'B', 'C'])` versus `(col == 'A') | (col == 'B') | (col == 'C')`. Beyond readability, what practical advantage does `isin()` provide when filtering for many values (e.g., 20+ stations)?\n",
    "2. Conditional assignment order: In Task 3b, why did we initialize all values to \"Midday\" before assigning rush hour categories? What would go wrong if you assigned categories in a different order, or didn't set a default?\n",
    "3. `query()` vs boolean indexing: The `query()` method in Task 3c could have been written with boolean indexing instead. When would you choose `query()` over boolean indexing? When might boolean indexing be preferable despite being more verbose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "##### Your Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "1. I prefer isin to |, as you could probably tell in my code, because yes, it is easier to read, but it's also easier to type and to debug. When you're using the pipeline for so many different options you're incredibly vulnerable to mistyping something and not noticing. Aside from that, using isin also allows you to link back to a separate list which could be fed from another function or analysis, which allows you to be more flexible in the problems you solve.\n",
    "2. Without initalizing originally, you could find yourself using incorrect logic or filtering incorrectly without noticing. If you assigned things in a different order, like if you were to use an if elif else loop in base python, you open yourself up to opportunities with logic errors where everything becomes morning rush or evening rush.\n",
    "3. I would choose to use query over boolean whenever I have more than two coonditions to meet. However, boolean may be preferable if you're wanting to loop conditions back to a different function that exists elsewhere in your code or if you're not familiar with query strings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "#### Follow-Up (Graduate Students Only)\n",
    "\n",
    "Pandas supports a variety of indexing paradigms, including bracket notation (`df['col']`), label-based indexing (`loc[]`), and position-based indexing (`iloc[]`). The lecture recommended using bracket notation only for columns, and loc/iloc for everything else. Explain the rationale: why is this approach better than using bracket notation for everything, even though `df[0:5]` technically works for row slicing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "##### Your Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "loc and iloc allow you to be clearer in what you're looking for, and it keeps it more consistent across your code. To someone who may not be super familiar with pandas, df[0:5] looks like columns 0 through 5, not rows. iloc and loc also allow you to reference specific columns and specific rows at the same time in two different formats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "### Problem 4: Temporal Analysis and Export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "Time-based patterns are crucial for understanding bike share usage. In this problem, you'll analyze when trips occur, how usage differs between rider types, and export filtered results. You'll use the datetime index you set in Problem 2 and the derived columns from Problems 2-3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "#### Task 4a: Identify Temporal Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "Use the datetime index to extract temporal components and identify usage patterns:\n",
    "\n",
    "1. Extract the *hour* from the index and use `value_counts()` to find the most popular hour for trips. Report the peak hour and how many trips occurred during that hour.\n",
    "2. Extract the *day name* from the index and use `value_counts()` to find the busiest day of the week. Report the day and number of trips.\n",
    "3. Sort the results from step 2 to show days in order from Monday to Sunday (not by trip count). Use `sort_index()`.\n",
    "\n",
    "Hint: Use `.dt.hour` and `.dt.day_name()` on the datetime index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10574 trips made at hour 17.\n",
      "16513 trips made on Wednesdays.\n",
      "started_at\n",
      "Monday       11531\n",
      "Tuesday      14970\n",
      "Wednesday    16513\n",
      "Thursday     16080\n",
      "Friday       13691\n",
      "Saturday     14427\n",
      "Sunday       12788\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Task 4a code here...\n",
    "values_at_hours = rides.index.hour.value_counts()\n",
    "number_of_trips = rides.index.hour.value_counts().max()\n",
    "hour = values_at_hours[values_at_hours == number_of_trips].index[0]\n",
    "\n",
    "print(f\"{number_of_trips} trips made at hour {hour}.\")\n",
    "\n",
    "values_at_days = rides.index.day_name().value_counts()\n",
    "number_of_trips_on_day = rides.index.day_name().value_counts().max()\n",
    "day = values_at_days[values_at_days == number_of_trips_on_day].index[0]\n",
    "\n",
    "#I would've much rather used dayofweek and had it index map to this list... lol\n",
    "#days_of_the_week = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "print(f\"{number_of_trips_on_day} trips made on {day}s.\")\n",
    "\n",
    "\n",
    "#this feels really clunky to me.. there's gotta be a better way. i would've used reidenx probably\n",
    "day_order_map = {'Monday': 0, 'Tuesday': 1, 'Wednesday': 2, 'Thursday': 3, 'Friday': 4, 'Saturday': 5, 'Sunday': 6}\n",
    "\n",
    "print(values_at_days.sort_index(key=lambda index_labels: index_labels.map(day_order_map)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "#### Task 4b: Compare Groups with groupby()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "Use `groupby()` (introduced in 07a) to compare trip characteristics across different groups:\n",
    "\n",
    "1. Calculate the average trip duration by rider type (`member_casual`). Which group takes longer trips on average?\n",
    "2. Calculate the average trip duration by bike type (`rideable_type`). Which bike type has the longest average trip?\n",
    "3. Count the number of trips by rider type using `groupby()` with `.size()`. Compare this with using `value_counts()` on the `member_casual` column - do they give the same result?\n",
    "\n",
    "Note: Use single-key groupby only (one column at a time)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group casual takes longer trips on average. 23.98 minutes as opposed to 11.98 minutes.\n",
      "Group classic_bike takes longer trips on average.\n",
      "\n",
      "Trips calculated with .size():\n",
      "member_casual\n",
      "casual    34686\n",
      "member    65314\n",
      "dtype: int64 \n",
      "and trips calculated with .value_counts():\n",
      "member_casual\n",
      "member    65314\n",
      "casual    34686\n",
      "Name: count, dtype: int64 \n",
      "are the same.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bp/bx13rkq17tg5ry84nxvc3kp40000gn/T/ipykernel_56722/1087086978.py:2: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  longest_trips = rides.groupby('member_casual')['trip_duration_min'].mean().max()\n",
      "/var/folders/bp/bx13rkq17tg5ry84nxvc3kp40000gn/T/ipykernel_56722/1087086978.py:3: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  shortest_trips = rides.groupby('member_casual')['trip_duration_min'].mean().min()\n",
      "/var/folders/bp/bx13rkq17tg5ry84nxvc3kp40000gn/T/ipykernel_56722/1087086978.py:4: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  group_breakdown = rides.groupby('member_casual')['trip_duration_min'].mean()\n",
      "/var/folders/bp/bx13rkq17tg5ry84nxvc3kp40000gn/T/ipykernel_56722/1087086978.py:10: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  longest_trips_bike = rides.groupby('rideable_type')['trip_duration_min'].mean().max()\n",
      "/var/folders/bp/bx13rkq17tg5ry84nxvc3kp40000gn/T/ipykernel_56722/1087086978.py:11: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  group_breakdown_bike = rides.groupby('rideable_type')['trip_duration_min'].mean()\n",
      "/var/folders/bp/bx13rkq17tg5ry84nxvc3kp40000gn/T/ipykernel_56722/1087086978.py:16: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  trips_with_size = rides.groupby('member_casual').size()\n"
     ]
    }
   ],
   "source": [
    "# Task 4b code here...\n",
    "longest_trips = rides.groupby('member_casual')['trip_duration_min'].mean().max()\n",
    "shortest_trips = rides.groupby('member_casual')['trip_duration_min'].mean().min()\n",
    "group_breakdown = rides.groupby('member_casual')['trip_duration_min'].mean()\n",
    "group = group_breakdown[group_breakdown == longest_trips].index[0]\n",
    "\n",
    "print(f\"Group {group} takes longer trips on average. {longest_trips:.2f} minutes as opposed to {shortest_trips:.2f} minutes.\")\n",
    "\n",
    "\n",
    "longest_trips_bike = rides.groupby('rideable_type')['trip_duration_min'].mean().max()\n",
    "group_breakdown_bike = rides.groupby('rideable_type')['trip_duration_min'].mean()\n",
    "group_bike = group_breakdown_bike[group_breakdown_bike == longest_trips_bike].index[0]\n",
    "\n",
    "print(f\"Group {group_bike} takes longer trips on average.\\n\")\n",
    "\n",
    "trips_with_size = rides.groupby('member_casual').size()\n",
    "trips_with_value_counts = rides['member_casual'].value_counts()\n",
    "print(f\"Trips calculated with .size():\\n{trips_with_size} \\nand trips calculated with .value_counts():\\n{trips_with_value_counts} \\nare the same.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "#### Task 4c: Filter, Sample, and Export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "Create a filtered dataset for weekend electric bike trips and export it:\n",
    "\n",
    "The provided code once again uses Path to create an `output` directory and constructs the full file path as `output/weekend_electric_trips.csv`. Use the `output_file` variable when calling `.to_csv()`.\n",
    "\n",
    "1. Filter for trips where `is_weekend == True` and `rideable_type == 'electric_bike'`\n",
    "2. Use `iloc[]` to select the first 1000 trips from this filtered dataset\n",
    "3. Use `reset_index()` to convert the datetime index back to a column (so it's included in the export)\n",
    "4. Export to CSV with filename `weekend_electric_trips.csv`, including only these columns: `started_at`, `ended_at`, `member_casual`, `trip_duration_min`, `time_of_day`\n",
    "5. Use `index=False` to avoid writing the default numeric index to the file\n",
    "\n",
    "After exporting, report how many total weekend electric bike trips existed before sampling to 1000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "##### Your Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trips is 13026.\n"
     ]
    }
   ],
   "source": [
    "# do not modify this setup code\n",
    "from pathlib import Path\n",
    "\n",
    "output_dir = Path('output')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "output_file = output_dir / 'weekend_electric_trips.csv'\n",
    "\n",
    "# Task 4c code here...\n",
    "# use the variable `output_file` as the filename for step 4\n",
    "filtered_trips = rides.query(\"is_weekend == True and rideable_type == 'electric_bike'\")\n",
    "first_1000 = filtered_trips.iloc[0:1000]\n",
    "first_1000 = first_1000.reset_index()\n",
    "first_1000.to_csv(output_file, columns=['started_at', 'ended_at', 'member_casual', 'trip_duration_min', 'time_of_day'], index=False)\n",
    "\n",
    "print(f\"Total trips is {filtered_trips['ride_id'].count()}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "#### Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {},
   "source": [
    "Reflect on this problem and answer the following questions:\n",
    "\n",
    "1. `groupby() conceptual model`: Explain in your own words what `groupby()` does. Use the phrase \"split-apply-combine\" in your explanation and describe what happens at each stage.\n",
    "2. `value_counts()` vs `groupby()`: In Task 4b.3, you compared two approaches for counting trips by rider type. When would you use `value_counts()` versus `groupby().size()`? Is there a situation where only one of them would work?\n",
    "3. Index management for export: In Task 4c, why did we use `reset_index()` before exporting? What would happen if you exported with the datetime index still in place and used `index=False`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {},
   "source": [
    "##### Your Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {},
   "source": [
    "1. Group by does exactly what you think it does, it groups the dataframe by the values in whatever column you specify. It splits the data into groups based on the values in your column, applies whatever aggregate function you specify, and combines those calculations back into an output.\n",
    "2. Value counts works without needing to group by, so it really just depends what you're getting this information for. If you're going to run other analysis, such as an average, group by would probably be the better option so you can have a consistent baseline. I can't think of a situation in which one wouldn't work, but that doesn't mean that it wouldn't be unnecessarily clunky.\n",
    "3. We reset the index so that started at would be included in the export. Otherwise, we would not have that data if index=False."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {},
   "source": [
    "#### Follow-Up (Graduate Students Only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "Compare `CSV` and _pickle_ formats for data storage and retrieval.\n",
    "\n",
    "Pickle is Python's built-in serialization format that saves Python objects exactly as they exist in memory, preserving all data types, structures, and metadata. Unlike CSV (which converts everything to text), pickle is binary (not human readable) and maintains the complete state of your DataFrame. Also, pickle files only work in Python, while CSV is universal. Read more in the [Pandas documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_pickle.html).\n",
    "\n",
    "The code below investigates an interesting pattern: Do riders take longer trips from scenic lakefront stations even during rush hours? This could indicate tourists or recreational riders using these popular locations for leisure trips during typical commute times. The analysis filters for trips over 15 minutes that started from lakefront stations during morning (7-9am) or evening (4-6pm) rush hours, sorted by duration to see the longest trips first.\n",
    "\n",
    "Run the code below, then answer the interpretation questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 310 long rush-hour trips from lakefront stations\n",
      "\n",
      "CSV file size: 40.27 KB\n",
      "Pickle file size: 21.18 KB\n",
      "Size difference: 19.09 KB\n",
      "\n",
      "Load time comparison:\n",
      "CSV:\n",
      "2.08 ms ± 142 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "\n",
      "Pickle:\n",
      "442 μs ± 31.4 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "\n",
      "Data types from CSV (without parse_dates):\n",
      "started_at             object\n",
      "ended_at               object\n",
      "start_station_name     object\n",
      "end_station_name       object\n",
      "member_casual          object\n",
      "rideable_type          object\n",
      "trip_duration_min     float64\n",
      "dtype: object\n",
      "\n",
      "Data types from Pickle:\n",
      "started_at            datetime64[ns]\n",
      "ended_at              datetime64[ns]\n",
      "start_station_name            object\n",
      "end_station_name              object\n",
      "member_casual               category\n",
      "rideable_type               category\n",
      "trip_duration_min            float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# the following lines were commented out since they were run in 4c\n",
    "# from pathlib import Path\n",
    "# output_dir = Path('output')\n",
    "\n",
    "csv_file = output_dir / 'lakefront_rush_trips.csv'\n",
    "pickle_file = output_dir / 'lakefront_rush_trips.pkl'\n",
    "\n",
    "# Filter for interesting pattern: Long trips (>15 min) during rush hours \n",
    "# from lakefront stations, sorted by duration\n",
    "lakefront_rush = (rides\n",
    "    .loc[(rides.index.hour.isin([7, 8, 9, 16, 17, 18]))]\n",
    "    .loc[(rides['start_station_name'].str.contains('Lake Shore|Lakefront', \n",
    "                                                    case=False, \n",
    "                                                    na=False))]\n",
    "    .loc[rides['trip_duration_min'] > 15]\n",
    "    .sort_values('trip_duration_min', ascending=False)\n",
    "    .head(1000)\n",
    "    .reset_index()\n",
    "    [['started_at', 'ended_at', 'start_station_name', 'end_station_name',\n",
    "      'member_casual', 'rideable_type', 'trip_duration_min']]\n",
    ")\n",
    "\n",
    "print(f\"Found {len(lakefront_rush)} long rush-hour trips from lakefront stations\")\n",
    "\n",
    "# Export to both formats\n",
    "lakefront_rush.to_csv(csv_file, index=False)\n",
    "lakefront_rush.to_pickle(pickle_file)\n",
    "\n",
    "# Compare file sizes\n",
    "csv_size = os.path.getsize(csv_file) / 1024  # Convert to KB\n",
    "pickle_size = os.path.getsize(pickle_file) / 1024\n",
    "print(f\"\\nCSV file size: {csv_size:.2f} KB\")\n",
    "print(f\"Pickle file size: {pickle_size:.2f} KB\")\n",
    "print(f\"Size difference: {abs(csv_size - pickle_size):.2f} KB\")\n",
    "\n",
    "# Compare load times\n",
    "print(\"\\nLoad time comparison:\")\n",
    "print(\"CSV:\")\n",
    "%timeit pd.read_csv(csv_file)\n",
    "print(\"\\nPickle:\")\n",
    "%timeit pd.read_pickle(pickle_file)\n",
    "\n",
    "# Check data type preservation\n",
    "# Note: CSV load without parse_dates loses datetime types\n",
    "csv_loaded = pd.read_csv(csv_file)\n",
    "pickle_loaded = pd.read_pickle(pickle_file)\n",
    "\n",
    "print(\"\\nData types from CSV (without parse_dates):\")\n",
    "print(csv_loaded.dtypes)\n",
    "print(\"\\nData types from Pickle:\")\n",
    "print(pickle_loaded.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {},
   "source": [
    "After running the code, answer these questions:\n",
    "\n",
    "1. Method chaining: The analysis uses method chaining with a specific formatting pattern:\n",
    "\n",
    "   ```python\n",
    "   result = (df\n",
    "       .method1()\n",
    "       .method2()\n",
    "       .method3()\n",
    "   )\n",
    "   ```\n",
    "\n",
    "   This wraps the entire chain in parentheses, allowing each method to appear on its own line without backslashes. Discuss why this makes formatting more readable, how it makes debugging easier, how it relates to seeing changes in the code with git diff, and what downsides heavy chaining might have.\n",
    "3. Data types: Compare the dtypes from CSV versus pickle. What types were preserved by pickle that were lost in CSV? Why is this preservation significant for subsequent analysis?\n",
    "4. Trade-offs: Given your observations about size, speed, and type preservation, when would you choose pickle over CSV for your work? When would CSV still be the better choice despite pickle's advantages?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "1. Allowing each method to sit on its own line makes reading a lot easier, and it's easier to debug since it's easier to separate the methods to find little things like an open parantheses. Heavy chaining at times is uneccessary and can make code difficult to read and work through.\n",
    "2. Pickle preserved categories and datetime objects, which could be important for someone else that is picking up where you left off, especially if they don't preview the data thouroughly.\n",
    "3. I would like choose pickle for instances that contain multiple data types that would be lost in a CSV and on projects that will be strictly python. Otherwise, CSV can be more user-friendly in the sense that if I use one language and I hand that work off to someone who uses another language, they can run with the CSV file, but not pickle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "Address the following questions in a markdown cell:\n",
    "\n",
    "1. NumPy vs Pandas\n",
    "   - What was the biggest conceptual shift moving from NumPy arrays to Pandas DataFrames?\n",
    "   - Which Pandas concept was most challenging: indexing (loc/iloc), missing data, datetime operations, or method chaining? How did you work through it?\n",
    "2. Real Data Experience\n",
    "   - How did working with real CSV data (with missing values, datetime strings, etc.) differ from hw2b's synthetic NumPy arrays?\n",
    "   - Based on this assignment, what makes Pandas well-suited for data analysis compared to pure NumPy?\n",
    "3. Learning & Application\n",
    "   - Which new skill from this assignment will be most useful for your own data work?\n",
    "   - On a scale of 1-10, how prepared do you feel to use Pandas for your own projects? What would increase that score?\n",
    "4. Feedback\n",
    "   - Time spent: ___ hours (breakdown optional)\n",
    "   - Most helpful part of the assignment: ___\n",
    "   - One specific improvement suggestion: ___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {},
   "source": [
    "### Your Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101",
   "metadata": {},
   "source": [
    "1. The biggest shift for me was just getting reaccustomed to pandas formatting and syntax. Indexing was the hardest concept for me because I couldn't keep loc and iloc straight, but once I realized the i stands for integer, it made that a lot easier.\n",
    "2. It definitely helped me understand what analysis can really be done from a coding perspective, and gave me ideas for our project. I've always preferred pandas for data analysis, largely because of datetime objects, but I think it just all around has a better formatting and internal layout for extensive analysis compared to NumPy.\n",
    "3. The query syntax was new for me, and I think that's a skill I'll definitely take with me. I feel pretty prepared (8/9) to use Pandas, especially since I've used it prior, but there's always room to learn more!!\n",
    "4.\n",
    "-Time spent: 11 hours\n",
    "-Most helpful part of the assignment: queries\n",
    "-One specific improvement suggestion: The subtasks under the subtasks under the problems are hard to follow and get lost. I almost didn't see the prompt to report in 4c. I would recommend adjusted formatting or just having 4a be 4a, not 4aa, 4ab, 4ac, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
